---
title: "p8105_hw3_yx2640"
author: "Elaine Xu"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
knitr::opts_chunk$set(
	fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

There are 134 aisles, most items are from fresh vegetables.
```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	) %>% 
	knitr::kable()
```


### Problem 2

__Load, tidy, and otherwise wrangle the data.__
```{r}
accel =
  read_csv(
		"./data/accel_data.csv"
		) %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    values_to = "activity_ct") %>%
  mutate(
    day_num = recode(day, "Sunday" = "7", "Saturday" = "6", "Friday" = "5", "Monday" = "1", "Tuesday" = "2", "Wednesday" = "3", "Thursday" = "4")) %>%
  arrange(week, day_num) %>%
  select(-day_num) %>%
  mutate(
    weekday_vs_weekend = case_when(
    day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday") ~ "weekday",
    day %in% c("Saturday","Sunday") ~ "weekend")) %>%
  mutate(
    day = as.factor(day))
```

This data collected five weeks of accelerometer data on a 63 year-old male with BMI 25. After tidy the original data, the resulting dataset has `r nrow(accel)` observations and `r ncol(accel)` variables. Day of the week is the only factor class variable. Other then `day`, variables `week`, `day_id` and `activity_ct` are class `numeric`, and variables `minute` and `weekday_vs_weekend` are class `character`. Variable `minute` collected 1440 minutes of each day in the `day` variable over five weeks.


__Aggregate accross minutes to create a total activity variable for each day, and create a table showing these totals.__
```{r}
total_active = accel %>%
	group_by(week, day) %>%
	count(sum(activity_ct)) %>%
  janitor::clean_names() 

total_active_table = total_active%>%
  mutate(
    day = as.character(day),
    day_num = recode(day, "Sunday" = "7", "Saturday" = "6", "Friday" = "5", "Monday" = "1", "Tuesday" = "2", "Wednesday" = "3", "Thursday" = "4")) %>%
  arrange(week, day_num) %>%
  select(-day_num, -n) %>%
  pivot_wider(names_from = day, values_from = sum_activity_ct)

knitr::kable(total_active_table)
```
There is no obvious trends just by looking at the table. For example, the Monday of the first week has the least activities, but in the following four weeks, Monday has a relatively higher activity counts compared with other days. There was little activity on the Saturday of the fourth and fifth weeks, but on the Saturday of the second week, activity counts reached its highest level of the week.


__Make a single-panel plot__
```{r}
accel %>%
  ggplot(aes(x = minute, y = activity_ct)) +
  geom_point(aes(color = day)) +
  labs(
    title = "24-hour activity time",
    x = "Minute of each day",
    y = "Activity of each minute",
    caption = "Data from the rnoaa package") +
  geom_smooth(alpha = 0.5)
```

From the graph we can see that on Saturday, this male has least activity compared with the other six days. In terms of point dispersion, there was more activity per minute on Friday and Wednesday. It can be seen from the figure that the amount of activity in the middle period is generally low, which indicates that the man is likely sleeping during this period.


### Problem 3

```{r}
data("ny_noaa")
```

__Data cleaning__

```{r}
my_ny_noaa = ny_noaa %>%
  separate(date, into = c("year", "month", "day"), convert = TRUE) %>%
  arrange(year, month) %>%
  mutate(
    prcp = as.numeric(prcp/10),
    tmax = as.numeric(tmax)/10,
    tmin = as.numeric(tmin)/10
  )

snow_obs = my_ny_noaa %>%
	count(snow) %>% 
	arrange(desc(n))
```

This data collected weather data from all New York state weather stations from January 1, 1981 through December 31, 2010. After tidy the original data, the resulting dataset has `r nrow(my_ny_noaa)` observations and `r ncol(my_ny_noaa)` variables. All variables are class `numeric` or `integer`. We separate the date into `year`, `month` and `day` so that the data can then be analyzed in terms of months. The unit for precipitation was "tenths of mm", same as `tmax` and `tmin`which maximum temperature and minimum temperature were both in the unit of "tenths of degrees C". We divided both data by ten, to let their unit can be easier compared with other.

We can find that there is a certain amount of missing data for variables other than station ID and date. The absence of such data may reduce statisticl power and the representativeness of the samples. Due to the missing data, we may not be able to have a more accurate analysis of the trend of temperature, snowfall and other data.

For snowfall, the most commonly observed value is 0. Since winter takes up only a quarter of the year, and most of the time there's no snow.


__Make a two-panel plot__
```{r}
Jan_vs_July = my_ny_noaa %>%
  filter(month %in% c("1", "7")) %>%
  group_by(id, year, month) %>%
  summarise(average_tmax = mean(tmax))

  ggplot(Jan_vs_July, aes(x = year, y = average_tmax)) +
  geom_point() +
  geom_smooth() +
  facet_grid(. ~ month) +
  labs(
    title = "Average max temperature in January and in July in each station across years",
    x = "year",
    y = "average maximum temperature (C)",
    caption = "Data from the rnoaa package")
```

As can be seen from the figure above, the highest temperature in January is much lower than the highest temperature in July. This is because January is still winter and the temperature is much lower than July, which is in summer. By looking at the overall trend, the highest temperatures in January vary widely from year to year. The fluctuation of temperature is wavy, decreasing year by year and then beginning to rise year by year.The July maximum temperatures were a bit less variable. There is outliers showing in July where the temperature is abnormally low, this could be because the monitoring station recorded data wrong.


__Make a two-panel plot showing (i) tmax vs tmin for the full dataset__


















